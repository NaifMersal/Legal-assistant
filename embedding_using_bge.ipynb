{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k6liYPDhnSkt",
        "outputId": "597bae93-ff9e-42a7-b7f6-e525876c4181"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.1.1)\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.12.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.1)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (2.8.0+cu126)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.16.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (0.35.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (11.3.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.15.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (25.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.10)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.10.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.3)\n",
            "Downloading faiss_cpu-1.12.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (31.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m82.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.12.0\n"
          ]
        }
      ],
      "source": [
        "!pip install sentence-transformers faiss-cpu transformers datasets tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ZZ_MLhECnzq2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/naif/miniconda3/envs/legal-assistant/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "PVv1Uxp-oFDy"
      },
      "outputs": [],
      "source": [
        "def extract_articles_v2(dataset):\n",
        "    articles = []\n",
        "    for category, subcats in dataset.items():\n",
        "        for subcat, systems in subcats.items():\n",
        "            for system_name, system_data in systems.items():\n",
        "                brief = system_data.get(\"brief\", \"\")\n",
        "                metadata = system_data.get(\"metadata\", {})\n",
        "                parts = system_data.get(\"parts\", {})\n",
        "                for part_name, part_articles in parts.items():\n",
        "                    for article in part_articles:\n",
        "                        articles.append({\n",
        "                            \"category\": category,\n",
        "                            \"sub_category\": subcat,\n",
        "                            \"system\": system_name,\n",
        "                            \"part\": part_name,\n",
        "                            \"brief\": brief,\n",
        "                            \"metadata\": metadata,\n",
        "                            \"id\": article.get(\"id\"),\n",
        "                            \"title\": article.get(\"Article_Title\"),\n",
        "                            \"status\": article.get(\"status\"),\n",
        "                            \"text\": article.get(\"Article_Text\")\n",
        "                        })\n",
        "    return articles\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ssGpFRAEoWyg",
        "outputId": "02bd08f9-c4a5-4358-a217-dc75792b6900"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Total Articles Extracted: 16371\n",
            "{\n",
            "  \"category\": \"أنظمة عادية\",\n",
            "  \"sub_category\": \"الأمن الداخلي والأحوال المدنية والأنظمة الجنائية\",\n",
            "  \"system\": \"نظام مكافحة غسل الأموال\",\n",
            "  \"part\": \"main\",\n",
            "  \"brief\": \"يتضمن النظام:\\r\\nالمقصود بالعبارات والألفاظ الواردة بالنظام. الأفعال التي يعد مرتكبها مرتكبًا جريمة غسل الأموال – ما يجب على المؤسسات المالية وغير المالية اتخاذه من إجراءات حيال مرتكب جريمة غسل الأموال – البرامج التي تضعها المؤسسات المالية وغير المالية لمكافحة عمليات غسل الأموال – وحدة مكافحة غسل الأموال – عقوبة مرتكب جريمة غسل الأموال.\",\n",
            "  \"metadata\": {\n",
            "    \"الاسم\": \"نظام مكافحة غسل الأموال\",\n",
            "    \"تاريخ الإصدار\": \"1433/05/11 هـ  الموافق : 03/04/2012 مـ\",\n",
            "    \"تاريخ النشر\": \"1433/08/02  هـ الموافق : 22/06/2012 مـ\",\n",
            "    \"الحالة\": \"لاغي\",\n",
            "    \"أدوات إصدار النظام\": [\n",
            "      {\n",
            "        \"text\": \"مرسوم ملكي رقم م / 31 بتاريخ 11 / 5 / 1433\",\n",
            "        \"url\": \"https://laws.boe.gov.sa/BoeLaws/Laws/Viewer/9ec732e6-9bbf-4fda-8a61-a9ae00c3c014?lawId=4a8842df-9cd1-4ee7-bf97-a9a700f180d4\"\n",
            "      },\n",
            "      {\n",
            "        \"text\": \"قرار مجلس الوزراء رقم 145 بتاريخ 10 / 5 / 1433\",\n",
            "        \"url\": \"https://laws.boe.gov.sa/BoeLaws/Laws/Viewer/c7630874-3862-4a17-97f9-a9ae00c3d573?lawId=4a8842df-9cd1-4ee7-bf97-a9a700f180d4\"\n",
            "      }\n",
            "    ]\n",
            "  },\n",
            "  \"id\": 604,\n",
            "  \"title\": \"المادة الأولى\",\n",
            "  \"status\": \"Canceled\",\n",
            "  \"text\": \"يقصد بالألفاظ والعبارات الآتية - أينما وردت في هذا النظام - المعاني الموضحة أمام كل منها، ما لم يقتض السياق خلاف ذلك :\\n1 - غسل الأموال :\\nارتكاب أي فعل أو الشروع فيه، يقصد من ورائه إخفاء أو تمويه أصل حقيقة أموال مكتسبة خلافاً للشرع أو النظام وجعلها تبدو كأنها مشروعة المصدر.\\n2 - الأموال :\\nالأصول أو الممتلكات أيًّا كانت قيمتها أو نوعها مادية أو غير مادية، ملموسة أو غير ملموسة، منقولة أو غير منقولة، والوثائق والصكوك والمستندات أيًّا كان شكلها بما في ذلك النظم الإلكترونية أو الرقمية والائتمانات المصرفية التي تدل على ملكية أو مصلحة فيها بما في ذلك على سبيل المثال لا الحصر جميع أنواع الشيكات والحوالات والأسهم والأوراق المالية والسندات والكمبيالات وخطابات الاعتماد.\\n3 - المتحصلات :\\nأي مال مستمد أو حصل عليه بطريق مباشر أو غير مباشر من ارتكاب جريمة من الجرائم المعاقب عليها وفقاً لأحكام الشريعة أو هذا النظام أو تم تحويله أو تبديله كليًّا أو جزئيًّا إلى أصول أو ممتلكات أو عائدات استثمارية.\\n4 ـ الوسائط :\\nكل ما استخدم أو أعد للاستخدام بأي شكل في ارتكاب جريمة من الجرائم المعاقب عليها وفقاً لأحكام الشريعة أو هذا النظام.\\n5 ـ المؤسسات المالية:\\nأي منشأة في المملكة تزاول واحداً أو أكثر من الأنشطة المصرفية وتحويل الأموال وتبديل العملات والاستثمار وأعمال الأوراق المالية والتأمين والتمويل، وتوضح اللائحة التنفيذية لهذا النظام الأنشطة المالية التي تزاولها هذه المنشأة.\\n6 - الأعمال والمهن غير المالية المحددة :\\nأي منشأة في المملكة تزاول واحداً أو أكثر من الأنشطة التجارية أو المهنية، وتوضح اللائحة التنفيذية لهذا النظام أنواع الأعمال والمهن غير المالية المحددة المزاولة في المملكة.\\n7 - المنظمات غير الهادفة للربح :\\nكل كيان قانوني يقوم بجمع أو تلقي أو صرف أموال لأغراض خيرية أو دينية أو ثقافية أو تعليمية أو اجتماعية أو تضامنية أو للقيام بأعمال أخرى من الأعمال الخيرية.\\n8 - العملية :\\nكل تصرف في الأموال أو الممتلكات أو المتحصلات النقدية أو العينية. ويشمل على سبيل المثال : الإيداع ، والسحب، والتحويل، والبيع، والشراء، والإقراض، والمبادلة أو استعمال خزائن الإيداع ونحوها مما تحدده اللائحة التنفيذية لهذا النظام.\\n9 - النشاط الإجرامي والجريمة الأصلية:\\nأي نشاط يشكل جريمة معاقباً عليها وفق الشرع أو النظام .\\n10 - الحجز التحفظي :\\nالحظر المؤقت على نقل الأموال والمتحصلات أو تحويلها أو تبديلها أو التصرف فيها أو تحريكها، أو وضع اليد عليها أو حجزها بصورة مؤقتة، استناداً إلى أمر صادر من محكمة أو سلطة مختصة بذلك.\\n11 - المصادرة :\\nالتجريد والحرمان الدائمان من الأموال أو المتحصلات أو الوسائط المستخدمة في الجريمة بناءً على حكم قضائي صادر من محكمة مختصة.\\n12 - الجهة الرقابية :\\nالجهة الحكومية المختصة بمنح التراخيص للمؤسسات المالية والأعمال والمهن غير المالية المحددة والمنظمات غير الهادفة للربح والمختصة كذلك بالرقابة أو الإشراف على تلك الجهات.\\n13 - السلطة المختصة :\\nكافة السلطات الإدارية وسلطات إنفاذ النظـام والجهات الرقابية المرتبطة بمكافحة غسل الأموال.\\n14 - الشخصية ذات الصفة الاعتبارية:\\nالهيئات التجارية أو المؤسسات أو الكيانات أو الشركات أو الجمعيات أو أي جهة مشابهة تستطيع إقامة علاقة عمل دائمة أو امتلاك أصول.\"\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "with open(\"data/saudi_laws_scraped.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "articles = extract_articles_v2(data)\n",
        "print(f\"✅ Total Articles Extracted: {len(articles)}\")\n",
        "print(json.dumps(articles[604], indent=2, ensure_ascii=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['يكون\\nعلم الدولة\\nكما يلي :\\nأ  - لونه أخضر.\\nب - عرضه يساوي ثلثي طوله.\\nج - تتوسطه كلمة : (لا إله إلا الله محمد رسول الله) تحتها سيف مسلول، ولا ينكس العلم أبدا.\\nويبين  النظام  الأحكام المتعلقة به.']"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "[article['text'] for i, article in  enumerate(articles) if article['id'] == 2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fPCVD52YodDx",
        "outputId": "f982685b-e2fd-4884-8760-447bcf4c3a29"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Corpus built with 16371 documents\n",
            "\n",
            "--- Example 1 ---\n",
            "Law Title: المادة الأولى\n",
            "\n",
            "Law Brief: يتضمن العناوين التالية: المبادئ العامة، نظام الحكم، مقومات المجتمع السعودي، المبادئ الاقتصادية، الحقوق والواجبات، سلطات الدولة، الشئون المالية، أحكام عامة.\n",
            "\n",
            "Law Text: المملكة العربية السعودية، دولة عربية إسلامية، ذات\n",
            "سيادة تامة\n",
            "، دينها\n",
            "الإسلام\n",
            "، ودستورها\n",
            "كتاب الله تعالى\n",
            "وسنة رسوله صلى الله عليه وسلم. ولغتها هي اللغة العربية، وعاصمتها مدينة الرياض.\n",
            "\n",
            "Law Metadata...\n",
            "\n",
            "--- Example 2 ---\n",
            "Law Title: المادة الثانية\n",
            "\n",
            "Law Brief: يتضمن العناوين التالية: المبادئ العامة، نظام الحكم، مقومات المجتمع السعودي، المبادئ الاقتصادية، الحقوق والواجبات، سلطات الدولة، الشئون المالية، أحكام عامة.\n",
            "\n",
            "Law Text: عيدا الدولة، هما عيدا الفطر والأضحى، وتقويمها، هو\n",
            "التقويم الهجري.\n",
            "\n",
            "Law Metadata: الاسم: النظام الأساسي للحكم تاريخ الإصدار: 1412/08/27 هـ  الموافق : 01/03/1992 مـ تاريخ النشر: 1412/09/02  هـ المو...\n"
          ]
        }
      ],
      "source": [
        "def build_corpus(articles):\n",
        "    corpus = []\n",
        "    for art in articles:\n",
        "        title = art.get(\"title\", \"\").strip()\n",
        "        brief = art.get(\"brief\", \"\").strip()\n",
        "        text = art.get(\"text\", \"\").strip()\n",
        "        \n",
        "        # Format metadata as \"key: value\" pairs\n",
        "        meta = art.get(\"metadata\", {})\n",
        "        meta_str = \" \".join(f\"{k}: {v}\" for k, v in meta.items() if v)\n",
        "\n",
        "        # Combine elements with clean formatting\n",
        "        parts = [\n",
        "            f\"Law Title: {title}\" if title else \"\",\n",
        "            f\"Law Brief: {brief}\" if brief else \"\",\n",
        "            f\"Law Text: {text}\" if text else \"\",\n",
        "            f\"Law Metadata: {meta_str}\" if meta_str else \"\",\n",
        "        ]\n",
        "\n",
        "        # Filter out empty parts and join with double newlines for clarity\n",
        "        entry = \"\\n\\n\".join(filter(None, parts)).strip()\n",
        "        corpus.append(entry)\n",
        "\n",
        "    return corpus\n",
        "\n",
        "\n",
        "corpus = build_corpus(articles)\n",
        "print(f\"✅ Corpus built with {len(corpus)} documents\")\n",
        "for i in range(2):\n",
        "    print(f\"\\n--- Example {i+1} ---\\n{corpus[i][:400]}...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "embed_model = SentenceTransformer(\"BAAI/bge-m3\", device=\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-JO4KME6ohFU",
        "outputId": "f1e8c107-2872-4e4e-dc99-d28d30b711fd"
      },
      "outputs": [],
      "source": [
        "def embed_corpus(corpus, embed_model, batch_size=128):\n",
        "    num_items = len(corpus)\n",
        "    dim = embed_model.get_sentence_embedding_dimension()\n",
        "    embeddings = np.zeros((num_items, dim), dtype=np.float32)\n",
        "\n",
        "    for start in tqdm(range(0, num_items, batch_size)):\n",
        "        end = min(start + batch_size, num_items)\n",
        "        batch = corpus[start:end]\n",
        "        embeddings[start:end] = embed_model.encode(batch, show_progress_bar=False, convert_to_numpy=True, normalize_embeddings=True)\n",
        "\n",
        "    return embeddings\n",
        "\n",
        "def save_faiss_index(embeddings, index_path=\"m3_legal_faiss.index\"):\n",
        "    dim = embeddings.shape[1]\n",
        "    base_index = faiss.IndexFlatIP(dim)\n",
        "    index = faiss.IndexIDMap(base_index)\n",
        "    ids = np.arange(embeddings.shape[0])\n",
        "    index.add_with_ids(embeddings, ids)\n",
        "    faiss.write_index(index, index_path)\n",
        "    return index\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "embed_corpus(corpus, embed_model)\n",
        "embeddings = embed_corpus(corpus, embed_model)\n",
        "print(f\"✅ Embeddings shape: {embeddings.shape}\")\n",
        "index = save_faiss_index(embeddings, index_path=\"legal_faiss_brief.index\")\n",
        "print(\"✅ FAISS index saved as 'legal_faiss_brief.index'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3KIGrRm0vzxh",
        "outputId": "01e205d6-b03e-492c-9463-b62054c77849"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "🔹 Result ID 0 (score=0.578)\n",
            "المادة الأولى - المملكة العربية السعودية، دولة عربية إسلامية، ذات\n",
            "سيادة تامة\n",
            "، دينها\n",
            "الإسلام\n",
            "، ودستورها\n",
            "كتاب الله تعالى\n",
            "وسنة رسوله صلى الله عليه وسلم. ولغتها هي اللغة العربية، وعاصمتها مدينة الرياض. الاسم: النظام الأساسي للحكم تاريخ الإصدار: 1412/08/27 هـ  الموافق : 01/03/1992 مـ تاريخ النشر: 1412/09/02  هـ الموافق : 06/03/1992 مـ الحالة: ساري أدوات إصدار النظام: [{'text': 'أمر ملكي رقم أ/90 بتاري...\n",
            "\n",
            "🔹 Result ID 2 (score=0.562)\n",
            "المادة الثالثة - يكون\n",
            "علم الدولة\n",
            "كما يلي :\n",
            "أ  - لونه أخضر.\n",
            "ب - عرضه يساوي ثلثي طوله.\n",
            "ج - تتوسطه كلمة : (لا إله إلا الله محمد رسول الله) تحتها سيف مسلول، ولا ينكس العلم أبدا.\n",
            "ويبين  النظام  الأحكام المتعلقة به. الاسم: النظام الأساسي للحكم تاريخ الإصدار: 1412/08/27 هـ  الموافق : 01/03/1992 مـ تاريخ النشر: 1412/09/02  هـ الموافق : 06/03/1992 مـ الحالة: ساري أدوات إصدار النظام: [{'text': 'أمر ملكي رقم...\n",
            "\n",
            "🔹 Result ID 1 (score=0.400)\n",
            "المادة الثانية - عيدا الدولة، هما عيدا الفطر والأضحى، وتقويمها، هو\n",
            "التقويم الهجري. الاسم: النظام الأساسي للحكم تاريخ الإصدار: 1412/08/27 هـ  الموافق : 01/03/1992 مـ تاريخ النشر: 1412/09/02  هـ الموافق : 06/03/1992 مـ الحالة: ساري أدوات إصدار النظام: [{'text': 'أمر ملكي رقم أ/90 بتاريخ 27 / 8 / 1412', 'url': 'https://laws.boe.gov.sa/BoeLaws/Laws/Viewer/0c1fa9f6-703e-4a93-ab8b-a6d7ad40628b?lawId=16b...\n",
            "\n",
            "🔹 Result ID 5 (score=0.370)\n",
            "المادة السادسة - يبايع المواطنون\n",
            "الملك\n",
            "على\n",
            "كتاب الله تعالى\n",
            "، وسنة رسوله، وعلى السمع والطاعة في العسر واليسر والمنشط والمكره. الاسم: النظام الأساسي للحكم تاريخ الإصدار: 1412/08/27 هـ  الموافق : 01/03/1992 مـ تاريخ النشر: 1412/09/02  هـ الموافق : 06/03/1992 مـ الحالة: ساري أدوات إصدار النظام: [{'text': 'أمر ملكي رقم أ/90 بتاريخ 27 / 8 / 1412', 'url': 'https://laws.boe.gov.sa/BoeLaws/Laws/Viewer/0c1f...\n",
            "\n",
            "🔹 Result ID -1 (score=-340282346638528859811704183484516925440.000)\n",
            "المادة الثالثة عشرة - تنشر الترتيبات في الجريدة الرسمية، ويعمل بها من تاريخ نشرها. الاسم: الترتيبات التنظيمية لمركز الفعاليات (المركز الوطني للفعاليات) تاريخ الإصدار: 1441/07/15 هـ  الموافق : 10/03/2020 مـ تاريخ النشر: لم يتم تحديد تاريخ النشر الحالة: جاري العمل على النظام أدوات إصدار النظام: [{'text': 'قرار مجلس الوزراء رقم (471) وتاريخ 1441/7/15هـ', 'url': 'https://laws.boe.gov.sa/BoeLaws/Laws/V...\n"
          ]
        }
      ],
      "source": [
        "filtered_indices = np.array([0, 1,2, 5 ], dtype=np.int64)\n",
        "selector = faiss.IDSelectorArray(filtered_indices)\n",
        "def retrieve(query, top_k=5):\n",
        "    q_emb = embed_model.encode([query], convert_to_numpy=True, normalize_embeddings=True)\n",
        "    D, I = index.search(q_emb, top_k, params=faiss.SearchParameters(sel=selector)\n",
        ")\n",
        "    results = [(i, float(D[0][j])) for j, i in enumerate(I[0])]\n",
        "    return results\n",
        "query = \"ما هو شعار الدولة السعودية؟\"\n",
        "results = retrieve(query)\n",
        "for i, (idx, score) in enumerate(results, 1):\n",
        "    print(f\"\\n🔹 Result ID {idx} (score={score:.3f})\\n{corpus[idx][:400]}...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Laws corpus built with 517 entries\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'Law Name:النظام الأساسي للحكم\\nLaw Summary:يتضمن العناوين التالية: المبادئ العامة، نظام الحكم، مقومات المجتمع السعودي، المبادئ الاقتصادية، الحقوق والواجبات، سلطات الدولة، الشئون المالية، أحكام عامة.'"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def build_law_corpus(data):\n",
        "    laws_text = []\n",
        "    # Traverse the 3-level hierarchy\n",
        "    for main_cat_name, sub_categories in data.items():\n",
        "        for sub_cat_name, laws in sub_categories.items():\n",
        "            for law_title, law_data in laws.items():\n",
        "                laws_text.append(\"Law Name:\"+ law_title+\"\\nLaw Summary:\"+law_data['brief'] )\n",
        "                \n",
        "    return laws_text\n",
        "\n",
        "laws_corpus = build_law_corpus(data)\n",
        "print(f\"✅ Laws corpus built with {len(laws_corpus)} entries\")\n",
        "laws_corpus[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 5/5 [00:03<00:00,  1.54it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Laws corpus embeddings shape: (517, 1024)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "embeddings = embed_corpus(laws_corpus, embed_model)\n",
        "print(f\"✅ Laws corpus embeddings shape: {embeddings.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "law_index = save_faiss_index(embeddings, index_path=\"laws_legal_faiss.index\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_parts_corpus(data, max_tokens=512, chunk_overlap=50, tokenizer_func=None):\n",
        "    def count_tokens(text):\n",
        "        \"\"\"Simple token counting - you might want to use a proper tokenizer\"\"\"\n",
        "        return len(tokenizer_func(text, add_special_tokens=False))\n",
        "    \n",
        "    def chunk_text(text, max_tokens, overlap):\n",
        "        \"\"\"Split text into chunks based on token count while preserving context\"\"\"\n",
        "        sentences = text.split('. ')\n",
        "        chunks = []\n",
        "        current_chunk = []\n",
        "        current_token_count = 0\n",
        "        \n",
        "        for sentence in sentences:\n",
        "            sentence_tokens = count_tokens(sentence)\n",
        "            \n",
        "            # If adding this sentence would exceed the limit\n",
        "            if current_token_count + sentence_tokens > max_tokens and current_chunk:\n",
        "                # Save current chunk\n",
        "                chunks.append('. '.join(current_chunk).strip())\n",
        "                \n",
        "                # Start new chunk with overlap\n",
        "                if overlap > 0:\n",
        "                    # Calculate how many sentences to keep for overlap\n",
        "                    overlap_text = '. '.join(current_chunk[-3:]).strip()  # Rough overlap\n",
        "                    current_chunk = [overlap_text] if overlap_text else []\n",
        "                    current_token_count = count_tokens(overlap_text) if overlap_text else 0\n",
        "                else:\n",
        "                    current_chunk = []\n",
        "                    current_token_count = 0\n",
        "            \n",
        "            current_chunk.append(sentence)\n",
        "            current_token_count += sentence_tokens\n",
        "        \n",
        "        # Add the last chunk if it has content\n",
        "        if current_chunk:\n",
        "            chunks.append('. '.join(current_chunk).strip())\n",
        "        \n",
        "        return chunks\n",
        "\n",
        "    def format_article(article):\n",
        "        title = article.get(\"Article_Title\", \"\").strip()\n",
        "        text = article.get(\"Article_Text\", \"\").strip()\n",
        "        parts = []\n",
        "        if title:\n",
        "            parts.append(f\"Article Title: {title}\")\n",
        "        if text:\n",
        "            parts.append(f\"Article Text: {text}\")\n",
        "        return \"\\n\".join(parts)\n",
        "    \n",
        "    corpus = []  # Will store dicts instead of plain text\n",
        "    id_to_part_id = {}  \n",
        "    chunk_id = 0\n",
        "    # Traverse the 3-level hierarchy\n",
        "    for main_cat_name, sub_categories in data.items():\n",
        "        for sub_cat_name, laws in sub_categories.items():\n",
        "            for law_title, law_data in laws.items():\n",
        "                parts = law_data.get(\"parts\", {})\n",
        "                brief = law_data.get(\"brief\", \"\")\n",
        "                law_text = \"Law Title: \" + law_title + \"\\n\" + \"Law Brief: \" + brief\n",
        "                \n",
        "                for part_name, part_articles in parts.items():\n",
        "                    # Build the part context\n",
        "                    if part_name != \"main\":\n",
        "                        context_lines = [law_text, f\"Part Name: {part_name}\"]\n",
        "                    else:\n",
        "                        context_lines = [law_text]\n",
        "                    \n",
        "                    context_lines.extend([format_article(article) for article in part_articles])\n",
        "                    full_text = \"\\n\".join(context_lines)\n",
        "                    \n",
        "                    part_id = law_title + \"|\" + part_name \n",
        "                   \n",
        "                    # Check token count\n",
        "                    token_count = count_tokens(full_text)\n",
        "                    if token_count > max_tokens:\n",
        "                        # Chunk the text and track chunk indices\n",
        "                        chunked_texts = chunk_text(full_text, max_tokens, chunk_overlap)\n",
        "                        \n",
        "                        for idx, chunk in enumerate(chunked_texts):\n",
        "                            # Add chunk to corpus with unique ID\n",
        "                            corpus.append(chunk)\n",
        "                            id_to_part_id[chunk_id] = part_id\n",
        "                            chunk_id += 1\n",
        "                    else:\n",
        "                        # Create unique ID for non-chunked part\n",
        "                        corpus.append(full_text)\n",
        "                        id_to_part_id[chunk_id] = part_id\n",
        "                        chunk_id += 1\n",
        "                        \n",
        "\n",
        "\n",
        "    return {\n",
        "        \"corpus\": corpus,  \n",
        "        \"corpus_id_to_part_id\": id_to_part_id  \n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "8192"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "embed_model.tokenizer.model_max_length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Parts corpus built with 2050 entries\n"
          ]
        }
      ],
      "source": [
        "parts_data = build_parts_corpus(data, max_tokens=8192, chunk_overlap=50, tokenizer_func=embed_model.tokenizer.encode)\n",
        "corpus = parts_data['corpus']\n",
        "print(f\"✅ Parts corpus built with {len(corpus)} entries\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2050"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(parts_data['corpus_id_to_part_id'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 129/129 [12:01<00:00,  5.60s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Parts corpus embeddings shape: (2050, 1024)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "embeddings = embed_corpus(corpus, embed_model, batch_size=16)\n",
        "print(f\"✅ Parts corpus embeddings shape: {embeddings.shape}\")\n",
        "index = save_faiss_index(embeddings, index_path=\"m3_legal_faiss_parts.index\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "json.dump(parts_data['corpus_id_to_part_id'], open(\"m3_corpus_id_to_part_id_parts.json\", \"w\", encoding=\"utf-8\"), ensure_ascii=False, indent=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "legal-assistant",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
