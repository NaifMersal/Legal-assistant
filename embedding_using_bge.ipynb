{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k6liYPDhnSkt",
        "outputId": "597bae93-ff9e-42a7-b7f6-e525876c4181"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.1.1)\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.12.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.1)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (2.8.0+cu126)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.16.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (0.35.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (11.3.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.15.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (25.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.10)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.10.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.3)\n",
            "Downloading faiss_cpu-1.12.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (31.4 MB)\n",
            "\u001b[2K   \u001b[90mโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m82.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.12.0\n"
          ]
        }
      ],
      "source": [
        "!pip install sentence-transformers faiss-cpu transformers datasets tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ZZ_MLhECnzq2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/naif/miniconda3/envs/legal-assistant/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "PVv1Uxp-oFDy"
      },
      "outputs": [],
      "source": [
        "def extract_articles_v2(dataset):\n",
        "    articles = []\n",
        "    for category, subcats in dataset.items():\n",
        "        for subcat, systems in subcats.items():\n",
        "            for system_name, system_data in systems.items():\n",
        "                brief = system_data.get(\"brief\", \"\")\n",
        "                metadata = system_data.get(\"metadata\", {})\n",
        "                parts = system_data.get(\"parts\", {})\n",
        "                for part_name, part_articles in parts.items():\n",
        "                    for article in part_articles:\n",
        "                        articles.append({\n",
        "                            \"category\": category,\n",
        "                            \"sub_category\": subcat,\n",
        "                            \"system\": system_name,\n",
        "                            \"part\": part_name,\n",
        "                            \"brief\": brief,\n",
        "                            \"metadata\": metadata,\n",
        "                            \"id\": article.get(\"id\"),\n",
        "                            \"title\": article.get(\"Article_Title\"),\n",
        "                            \"status\": article.get(\"status\"),\n",
        "                            \"text\": article.get(\"Article_Text\")\n",
        "                        })\n",
        "    return articles\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ssGpFRAEoWyg",
        "outputId": "02bd08f9-c4a5-4358-a217-dc75792b6900"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "โ Total Articles Extracted: 16371\n",
            "{\n",
            "  \"category\": \"ุฃูุธูุฉ ุนุงุฏูุฉ\",\n",
            "  \"sub_category\": \"ุงูุฃูู ุงูุฏุงุฎูู ูุงูุฃุญูุงู ุงููุฏููุฉ ูุงูุฃูุธูุฉ ุงูุฌูุงุฆูุฉ\",\n",
            "  \"system\": \"ูุธุงู ููุงูุญุฉ ุบุณู ุงูุฃููุงู\",\n",
            "  \"part\": \"main\",\n",
            "  \"brief\": \"ูุชุถูู ุงููุธุงู:\\r\\nุงูููุตูุฏ ุจุงูุนุจุงุฑุงุช ูุงูุฃููุงุธ ุงููุงุฑุฏุฉ ุจุงููุธุงู. ุงูุฃูุนุงู ุงูุชู ูุนุฏ ูุฑุชูุจูุง ูุฑุชูุจูุง ุฌุฑููุฉ ุบุณู ุงูุฃููุงู โ ูุง ูุฌุจ ุนูู ุงููุคุณุณุงุช ุงููุงููุฉ ูุบูุฑ ุงููุงููุฉ ุงุชุฎุงุฐู ูู ุฅุฌุฑุงุกุงุช ุญูุงู ูุฑุชูุจ ุฌุฑููุฉ ุบุณู ุงูุฃููุงู โ ุงูุจุฑุงูุฌ ุงูุชู ุชุถุนูุง ุงููุคุณุณุงุช ุงููุงููุฉ ูุบูุฑ ุงููุงููุฉ ูููุงูุญุฉ ุนูููุงุช ุบุณู ุงูุฃููุงู โ ูุญุฏุฉ ููุงูุญุฉ ุบุณู ุงูุฃููุงู โ ุนููุจุฉ ูุฑุชูุจ ุฌุฑููุฉ ุบุณู ุงูุฃููุงู.\",\n",
            "  \"metadata\": {\n",
            "    \"ุงูุงุณู\": \"ูุธุงู ููุงูุญุฉ ุบุณู ุงูุฃููุงู\",\n",
            "    \"ุชุงุฑูุฎ ุงูุฅุตุฏุงุฑ\": \"1433/05/11 ูู  ุงูููุงูู : 03/04/2012 ูู\",\n",
            "    \"ุชุงุฑูุฎ ุงููุดุฑ\": \"1433/08/02  ูู ุงูููุงูู : 22/06/2012 ูู\",\n",
            "    \"ุงูุญุงูุฉ\": \"ูุงุบู\",\n",
            "    \"ุฃุฏูุงุช ุฅุตุฏุงุฑ ุงููุธุงู\": [\n",
            "      {\n",
            "        \"text\": \"ูุฑุณูู ูููู ุฑูู ู / 31 ุจุชุงุฑูุฎ 11 / 5 / 1433\",\n",
            "        \"url\": \"https://laws.boe.gov.sa/BoeLaws/Laws/Viewer/9ec732e6-9bbf-4fda-8a61-a9ae00c3c014?lawId=4a8842df-9cd1-4ee7-bf97-a9a700f180d4\"\n",
            "      },\n",
            "      {\n",
            "        \"text\": \"ูุฑุงุฑ ูุฌูุณ ุงููุฒุฑุงุก ุฑูู 145 ุจุชุงุฑูุฎ 10 / 5 / 1433\",\n",
            "        \"url\": \"https://laws.boe.gov.sa/BoeLaws/Laws/Viewer/c7630874-3862-4a17-97f9-a9ae00c3d573?lawId=4a8842df-9cd1-4ee7-bf97-a9a700f180d4\"\n",
            "      }\n",
            "    ]\n",
            "  },\n",
            "  \"id\": 604,\n",
            "  \"title\": \"ุงููุงุฏุฉ ุงูุฃููู\",\n",
            "  \"status\": \"Canceled\",\n",
            "  \"text\": \"ููุตุฏ ุจุงูุฃููุงุธ ูุงูุนุจุงุฑุงุช ุงูุขุชูุฉ - ุฃูููุง ูุฑุฏุช ูู ูุฐุง ุงููุธุงู - ุงููุนุงูู ุงูููุถุญุฉ ุฃูุงู ูู ูููุงุ ูุง ูู ููุชุถ ุงูุณูุงู ุฎูุงู ุฐูู :\\n1 - ุบุณู ุงูุฃููุงู :\\nุงุฑุชูุงุจ ุฃู ูุนู ุฃู ุงูุดุฑูุน ูููุ ููุตุฏ ูู ูุฑุงุฆู ุฅุฎูุงุก ุฃู ุชูููู ุฃุตู ุญูููุฉ ุฃููุงู ููุชุณุจุฉ ุฎูุงูุงู ููุดุฑุน ุฃู ุงููุธุงู ูุฌุนููุง ุชุจุฏู ูุฃููุง ูุดุฑูุนุฉ ุงููุตุฏุฑ.\\n2 - ุงูุฃููุงู :\\nุงูุฃุตูู ุฃู ุงูููุชููุงุช ุฃูููุง ูุงูุช ูููุชูุง ุฃู ููุนูุง ูุงุฏูุฉ ุฃู ุบูุฑ ูุงุฏูุฉุ ููููุณุฉ ุฃู ุบูุฑ ููููุณุฉุ ูููููุฉ ุฃู ุบูุฑ ูููููุฉุ ูุงููุซุงุฆู ูุงูุตููู ูุงููุณุชูุฏุงุช ุฃูููุง ูุงู ุดูููุง ุจูุง ูู ุฐูู ุงููุธู ุงูุฅููุชุฑูููุฉ ุฃู ุงูุฑูููุฉ ูุงูุงุฆุชูุงูุงุช ุงููุตุฑููุฉ ุงูุชู ุชุฏู ุนูู ููููุฉ ุฃู ูุตูุญุฉ ูููุง ุจูุง ูู ุฐูู ุนูู ุณุจูู ุงููุซุงู ูุง ุงูุญุตุฑ ุฌููุน ุฃููุงุน ุงูุดููุงุช ูุงูุญูุงูุงุช ูุงูุฃุณูู ูุงูุฃูุฑุงู ุงููุงููุฉ ูุงูุณูุฏุงุช ูุงูููุจูุงูุงุช ูุฎุทุงุจุงุช ุงูุงุนุชูุงุฏ.\\n3 - ุงููุชุญุตูุงุช :\\nุฃู ูุงู ูุณุชูุฏ ุฃู ุญุตู ุนููู ุจุทุฑูู ูุจุงุดุฑ ุฃู ุบูุฑ ูุจุงุดุฑ ูู ุงุฑุชูุงุจ ุฌุฑููุฉ ูู ุงูุฌุฑุงุฆู ุงููุนุงูุจ ุนูููุง ูููุงู ูุฃุญูุงู ุงูุดุฑูุนุฉ ุฃู ูุฐุง ุงููุธุงู ุฃู ุชู ุชุญูููู ุฃู ุชุจุฏููู ูููููุง ุฃู ุฌุฒุฆูููุง ุฅูู ุฃุตูู ุฃู ููุชููุงุช ุฃู ุนุงุฆุฏุงุช ุงุณุชุซูุงุฑูุฉ.\\n4 ู ุงููุณุงุฆุท :\\nูู ูุง ุงุณุชุฎุฏู ุฃู ุฃุนุฏ ููุงุณุชุฎุฏุงู ุจุฃู ุดูู ูู ุงุฑุชูุงุจ ุฌุฑููุฉ ูู ุงูุฌุฑุงุฆู ุงููุนุงูุจ ุนูููุง ูููุงู ูุฃุญูุงู ุงูุดุฑูุนุฉ ุฃู ูุฐุง ุงููุธุงู.\\n5 ู ุงููุคุณุณุงุช ุงููุงููุฉ:\\nุฃู ููุดุฃุฉ ูู ุงูููููุฉ ุชุฒุงูู ูุงุญุฏุงู ุฃู ุฃูุซุฑ ูู ุงูุฃูุดุทุฉ ุงููุตุฑููุฉ ูุชุญููู ุงูุฃููุงู ูุชุจุฏูู ุงูุนููุงุช ูุงูุงุณุชุซูุงุฑ ูุฃุนูุงู ุงูุฃูุฑุงู ุงููุงููุฉ ูุงูุชุฃููู ูุงูุชููููุ ูุชูุถุญ ุงููุงุฆุญุฉ ุงูุชูููุฐูุฉ ููุฐุง ุงููุธุงู ุงูุฃูุดุทุฉ ุงููุงููุฉ ุงูุชู ุชุฒุงูููุง ูุฐู ุงูููุดุฃุฉ.\\n6 - ุงูุฃุนูุงู ูุงูููู ุบูุฑ ุงููุงููุฉ ุงููุญุฏุฏุฉ :\\nุฃู ููุดุฃุฉ ูู ุงูููููุฉ ุชุฒุงูู ูุงุญุฏุงู ุฃู ุฃูุซุฑ ูู ุงูุฃูุดุทุฉ ุงูุชุฌุงุฑูุฉ ุฃู ุงูููููุฉุ ูุชูุถุญ ุงููุงุฆุญุฉ ุงูุชูููุฐูุฉ ููุฐุง ุงููุธุงู ุฃููุงุน ุงูุฃุนูุงู ูุงูููู ุบูุฑ ุงููุงููุฉ ุงููุญุฏุฏุฉ ุงููุฒุงููุฉ ูู ุงูููููุฉ.\\n7 - ุงูููุธูุงุช ุบูุฑ ุงููุงุฏูุฉ ููุฑุจุญ :\\nูู ููุงู ูุงูููู ูููู ุจุฌูุน ุฃู ุชููู ุฃู ุตุฑู ุฃููุงู ูุฃุบุฑุงุถ ุฎูุฑูุฉ ุฃู ุฏูููุฉ ุฃู ุซูุงููุฉ ุฃู ุชุนููููุฉ ุฃู ุงุฌุชูุงุนูุฉ ุฃู ุชุถุงูููุฉ ุฃู ููููุงู ุจุฃุนูุงู ุฃุฎุฑู ูู ุงูุฃุนูุงู ุงูุฎูุฑูุฉ.\\n8 - ุงูุนูููุฉ :\\nูู ุชุตุฑู ูู ุงูุฃููุงู ุฃู ุงูููุชููุงุช ุฃู ุงููุชุญุตูุงุช ุงูููุฏูุฉ ุฃู ุงูุนูููุฉ. ููุดูู ุนูู ุณุจูู ุงููุซุงู : ุงูุฅูุฏุงุน ุ ูุงูุณุญุจุ ูุงูุชุญูููุ ูุงูุจูุนุ ูุงูุดุฑุงุกุ ูุงูุฅูุฑุงุถุ ูุงููุจุงุฏูุฉ ุฃู ุงุณุชุนูุงู ุฎุฒุงุฆู ุงูุฅูุฏุงุน ููุญููุง ููุง ุชุญุฏุฏู ุงููุงุฆุญุฉ ุงูุชูููุฐูุฉ ููุฐุง ุงููุธุงู.\\n9 - ุงููุดุงุท ุงูุฅุฌุฑุงูู ูุงูุฌุฑููุฉ ุงูุฃุตููุฉ:\\nุฃู ูุดุงุท ูุดูู ุฌุฑููุฉ ูุนุงูุจุงู ุนูููุง ููู ุงูุดุฑุน ุฃู ุงููุธุงู .\\n10 - ุงูุญุฌุฒ ุงูุชุญูุธู :\\nุงูุญุธุฑ ุงููุคูุช ุนูู ููู ุงูุฃููุงู ูุงููุชุญุตูุงุช ุฃู ุชุญููููุง ุฃู ุชุจุฏูููุง ุฃู ุงูุชุตุฑู ูููุง ุฃู ุชุญุฑูููุงุ ุฃู ูุถุน ุงููุฏ ุนูููุง ุฃู ุญุฌุฒูุง ุจุตูุฑุฉ ูุคูุชุฉุ ุงุณุชูุงุฏุงู ุฅูู ุฃูุฑ ุตุงุฏุฑ ูู ูุญููุฉ ุฃู ุณูุทุฉ ูุฎุชุตุฉ ุจุฐูู.\\n11 - ุงููุตุงุฏุฑุฉ :\\nุงูุชุฌุฑูุฏ ูุงูุญุฑูุงู ุงูุฏุงุฆูุงู ูู ุงูุฃููุงู ุฃู ุงููุชุญุตูุงุช ุฃู ุงููุณุงุฆุท ุงููุณุชุฎุฏูุฉ ูู ุงูุฌุฑููุฉ ุจูุงุกู ุนูู ุญูู ูุถุงุฆู ุตุงุฏุฑ ูู ูุญููุฉ ูุฎุชุตุฉ.\\n12 - ุงูุฌูุฉ ุงูุฑูุงุจูุฉ :\\nุงูุฌูุฉ ุงูุญููููุฉ ุงููุฎุชุตุฉ ุจููุญ ุงูุชุฑุงุฎูุต ูููุคุณุณุงุช ุงููุงููุฉ ูุงูุฃุนูุงู ูุงูููู ุบูุฑ ุงููุงููุฉ ุงููุญุฏุฏุฉ ูุงูููุธูุงุช ุบูุฑ ุงููุงุฏูุฉ ููุฑุจุญ ูุงููุฎุชุตุฉ ูุฐูู ุจุงูุฑูุงุจุฉ ุฃู ุงูุฅุดุฑุงู ุนูู ุชูู ุงูุฌูุงุช.\\n13 - ุงูุณูุทุฉ ุงููุฎุชุตุฉ :\\nูุงูุฉ ุงูุณูุทุงุช ุงูุฅุฏุงุฑูุฉ ูุณูุทุงุช ุฅููุงุฐ ุงููุธูุงู ูุงูุฌูุงุช ุงูุฑูุงุจูุฉ ุงููุฑุชุจุทุฉ ุจููุงูุญุฉ ุบุณู ุงูุฃููุงู.\\n14 - ุงูุดุฎุตูุฉ ุฐุงุช ุงูุตูุฉ ุงูุงุนุชุจุงุฑูุฉ:\\nุงูููุฆุงุช ุงูุชุฌุงุฑูุฉ ุฃู ุงููุคุณุณุงุช ุฃู ุงูููุงูุงุช ุฃู ุงูุดุฑูุงุช ุฃู ุงูุฌูุนูุงุช ุฃู ุฃู ุฌูุฉ ูุดุงุจูุฉ ุชุณุชุทูุน ุฅูุงูุฉ ุนูุงูุฉ ุนูู ุฏุงุฆูุฉ ุฃู ุงูุชูุงู ุฃุตูู.\"\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "with open(\"data/saudi_laws_scraped.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "articles = extract_articles_v2(data)\n",
        "print(f\"โ Total Articles Extracted: {len(articles)}\")\n",
        "print(json.dumps(articles[604], indent=2, ensure_ascii=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['ูููู\\nุนูู ุงูุฏููุฉ\\nููุง ููู :\\nุฃ  - ูููู ุฃุฎุถุฑ.\\nุจ - ุนุฑุถู ูุณุงูู ุซูุซู ุทููู.\\nุฌ - ุชุชูุณุทู ูููุฉ : (ูุง ุฅูู ุฅูุง ุงููู ูุญูุฏ ุฑุณูู ุงููู) ุชุญุชูุง ุณูู ูุณูููุ ููุง ูููุณ ุงูุนูู ุฃุจุฏุง.\\nููุจูู  ุงููุธุงู  ุงูุฃุญูุงู ุงููุชุนููุฉ ุจู.']"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "[article['text'] for i, article in  enumerate(articles) if article['id'] == 2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fPCVD52YodDx",
        "outputId": "f982685b-e2fd-4884-8760-447bcf4c3a29"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "โ Corpus built with 16371 documents\n",
            "\n",
            "--- Example 1 ---\n",
            "Law Title: ุงููุงุฏุฉ ุงูุฃููู\n",
            "\n",
            "Law Brief: ูุชุถูู ุงูุนูุงููู ุงูุชุงููุฉ: ุงููุจุงุฏุฆ ุงูุนุงูุฉุ ูุธุงู ุงูุญููุ ููููุงุช ุงููุฌุชูุน ุงูุณุนูุฏูุ ุงููุจุงุฏุฆ ุงูุงูุชุตุงุฏูุฉุ ุงูุญููู ูุงููุงุฌุจุงุชุ ุณูุทุงุช ุงูุฏููุฉุ ุงูุดุฆูู ุงููุงููุฉุ ุฃุญูุงู ุนุงูุฉ.\n",
            "\n",
            "Law Text: ุงูููููุฉ ุงูุนุฑุจูุฉ ุงูุณุนูุฏูุฉุ ุฏููุฉ ุนุฑุจูุฉ ุฅุณูุงููุฉุ ุฐุงุช\n",
            "ุณูุงุฏุฉ ุชุงูุฉ\n",
            "ุ ุฏูููุง\n",
            "ุงูุฅุณูุงู\n",
            "ุ ูุฏุณุชูุฑูุง\n",
            "ูุชุงุจ ุงููู ุชุนุงูู\n",
            "ูุณูุฉ ุฑุณููู ุตูู ุงููู ุนููู ูุณูู. ููุบุชูุง ูู ุงููุบุฉ ุงูุนุฑุจูุฉุ ูุนุงุตูุชูุง ูุฏููุฉ ุงูุฑูุงุถ.\n",
            "\n",
            "Law Metadata...\n",
            "\n",
            "--- Example 2 ---\n",
            "Law Title: ุงููุงุฏุฉ ุงูุซุงููุฉ\n",
            "\n",
            "Law Brief: ูุชุถูู ุงูุนูุงููู ุงูุชุงููุฉ: ุงููุจุงุฏุฆ ุงูุนุงูุฉุ ูุธุงู ุงูุญููุ ููููุงุช ุงููุฌุชูุน ุงูุณุนูุฏูุ ุงููุจุงุฏุฆ ุงูุงูุชุตุงุฏูุฉุ ุงูุญููู ูุงููุงุฌุจุงุชุ ุณูุทุงุช ุงูุฏููุฉุ ุงูุดุฆูู ุงููุงููุฉุ ุฃุญูุงู ุนุงูุฉ.\n",
            "\n",
            "Law Text: ุนูุฏุง ุงูุฏููุฉุ ููุง ุนูุฏุง ุงููุทุฑ ูุงูุฃุถุญูุ ูุชูููููุงุ ูู\n",
            "ุงูุชูููู ุงููุฌุฑู.\n",
            "\n",
            "Law Metadata: ุงูุงุณู: ุงููุธุงู ุงูุฃุณุงุณู ููุญูู ุชุงุฑูุฎ ุงูุฅุตุฏุงุฑ: 1412/08/27 ูู  ุงูููุงูู : 01/03/1992 ูู ุชุงุฑูุฎ ุงููุดุฑ: 1412/09/02  ูู ุงููู...\n"
          ]
        }
      ],
      "source": [
        "def build_corpus(articles):\n",
        "    corpus = []\n",
        "    for art in articles:\n",
        "        title = art.get(\"title\", \"\").strip()\n",
        "        brief = art.get(\"brief\", \"\").strip()\n",
        "        text = art.get(\"text\", \"\").strip()\n",
        "        \n",
        "        # Format metadata as \"key: value\" pairs\n",
        "        meta = art.get(\"metadata\", {})\n",
        "        meta_str = \" \".join(f\"{k}: {v}\" for k, v in meta.items() if v)\n",
        "\n",
        "        # Combine elements with clean formatting\n",
        "        parts = [\n",
        "            f\"Law Title: {title}\" if title else \"\",\n",
        "            f\"Law Brief: {brief}\" if brief else \"\",\n",
        "            f\"Law Text: {text}\" if text else \"\",\n",
        "            f\"Law Metadata: {meta_str}\" if meta_str else \"\",\n",
        "        ]\n",
        "\n",
        "        # Filter out empty parts and join with double newlines for clarity\n",
        "        entry = \"\\n\\n\".join(filter(None, parts)).strip()\n",
        "        corpus.append(entry)\n",
        "\n",
        "    return corpus\n",
        "\n",
        "\n",
        "corpus = build_corpus(articles)\n",
        "print(f\"โ Corpus built with {len(corpus)} documents\")\n",
        "for i in range(2):\n",
        "    print(f\"\\n--- Example {i+1} ---\\n{corpus[i][:400]}...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "embed_model = SentenceTransformer(\"BAAI/bge-m3\", device=\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-JO4KME6ohFU",
        "outputId": "f1e8c107-2872-4e4e-dc99-d28d30b711fd"
      },
      "outputs": [],
      "source": [
        "def embed_corpus(corpus, embed_model, batch_size=128):\n",
        "    num_items = len(corpus)\n",
        "    dim = embed_model.get_sentence_embedding_dimension()\n",
        "    embeddings = np.zeros((num_items, dim), dtype=np.float32)\n",
        "\n",
        "    for start in tqdm(range(0, num_items, batch_size)):\n",
        "        end = min(start + batch_size, num_items)\n",
        "        batch = corpus[start:end]\n",
        "        embeddings[start:end] = embed_model.encode(batch, show_progress_bar=False, convert_to_numpy=True, normalize_embeddings=True)\n",
        "\n",
        "    return embeddings\n",
        "\n",
        "def save_faiss_index(embeddings, index_path=\"m3_legal_faiss.index\"):\n",
        "    dim = embeddings.shape[1]\n",
        "    base_index = faiss.IndexFlatIP(dim)\n",
        "    index = faiss.IndexIDMap(base_index)\n",
        "    ids = np.arange(embeddings.shape[0])\n",
        "    index.add_with_ids(embeddings, ids)\n",
        "    faiss.write_index(index, index_path)\n",
        "    return index\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "embed_corpus(corpus, embed_model)\n",
        "embeddings = embed_corpus(corpus, embed_model)\n",
        "print(f\"โ Embeddings shape: {embeddings.shape}\")\n",
        "index = save_faiss_index(embeddings, index_path=\"legal_faiss_brief.index\")\n",
        "print(\"โ FAISS index saved as 'legal_faiss_brief.index'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3KIGrRm0vzxh",
        "outputId": "01e205d6-b03e-492c-9463-b62054c77849"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "๐น Result ID 0 (score=0.578)\n",
            "ุงููุงุฏุฉ ุงูุฃููู - ุงูููููุฉ ุงูุนุฑุจูุฉ ุงูุณุนูุฏูุฉุ ุฏููุฉ ุนุฑุจูุฉ ุฅุณูุงููุฉุ ุฐุงุช\n",
            "ุณูุงุฏุฉ ุชุงูุฉ\n",
            "ุ ุฏูููุง\n",
            "ุงูุฅุณูุงู\n",
            "ุ ูุฏุณุชูุฑูุง\n",
            "ูุชุงุจ ุงููู ุชุนุงูู\n",
            "ูุณูุฉ ุฑุณููู ุตูู ุงููู ุนููู ูุณูู. ููุบุชูุง ูู ุงููุบุฉ ุงูุนุฑุจูุฉุ ูุนุงุตูุชูุง ูุฏููุฉ ุงูุฑูุงุถ. ุงูุงุณู: ุงููุธุงู ุงูุฃุณุงุณู ููุญูู ุชุงุฑูุฎ ุงูุฅุตุฏุงุฑ: 1412/08/27 ูู  ุงูููุงูู : 01/03/1992 ูู ุชุงุฑูุฎ ุงููุดุฑ: 1412/09/02  ูู ุงูููุงูู : 06/03/1992 ูู ุงูุญุงูุฉ: ุณุงุฑู ุฃุฏูุงุช ุฅุตุฏุงุฑ ุงููุธุงู: [{'text': 'ุฃูุฑ ูููู ุฑูู ุฃ/90 ุจุชุงุฑู...\n",
            "\n",
            "๐น Result ID 2 (score=0.562)\n",
            "ุงููุงุฏุฉ ุงูุซุงูุซุฉ - ูููู\n",
            "ุนูู ุงูุฏููุฉ\n",
            "ููุง ููู :\n",
            "ุฃ  - ูููู ุฃุฎุถุฑ.\n",
            "ุจ - ุนุฑุถู ูุณุงูู ุซูุซู ุทููู.\n",
            "ุฌ - ุชุชูุณุทู ูููุฉ : (ูุง ุฅูู ุฅูุง ุงููู ูุญูุฏ ุฑุณูู ุงููู) ุชุญุชูุง ุณูู ูุณูููุ ููุง ูููุณ ุงูุนูู ุฃุจุฏุง.\n",
            "ููุจูู  ุงููุธุงู  ุงูุฃุญูุงู ุงููุชุนููุฉ ุจู. ุงูุงุณู: ุงููุธุงู ุงูุฃุณุงุณู ููุญูู ุชุงุฑูุฎ ุงูุฅุตุฏุงุฑ: 1412/08/27 ูู  ุงูููุงูู : 01/03/1992 ูู ุชุงุฑูุฎ ุงููุดุฑ: 1412/09/02  ูู ุงูููุงูู : 06/03/1992 ูู ุงูุญุงูุฉ: ุณุงุฑู ุฃุฏูุงุช ุฅุตุฏุงุฑ ุงููุธุงู: [{'text': 'ุฃูุฑ ูููู ุฑูู...\n",
            "\n",
            "๐น Result ID 1 (score=0.400)\n",
            "ุงููุงุฏุฉ ุงูุซุงููุฉ - ุนูุฏุง ุงูุฏููุฉุ ููุง ุนูุฏุง ุงููุทุฑ ูุงูุฃุถุญูุ ูุชูููููุงุ ูู\n",
            "ุงูุชูููู ุงููุฌุฑู. ุงูุงุณู: ุงููุธุงู ุงูุฃุณุงุณู ููุญูู ุชุงุฑูุฎ ุงูุฅุตุฏุงุฑ: 1412/08/27 ูู  ุงูููุงูู : 01/03/1992 ูู ุชุงุฑูุฎ ุงููุดุฑ: 1412/09/02  ูู ุงูููุงูู : 06/03/1992 ูู ุงูุญุงูุฉ: ุณุงุฑู ุฃุฏูุงุช ุฅุตุฏุงุฑ ุงููุธุงู: [{'text': 'ุฃูุฑ ูููู ุฑูู ุฃ/90 ุจุชุงุฑูุฎ 27 / 8 / 1412', 'url': 'https://laws.boe.gov.sa/BoeLaws/Laws/Viewer/0c1fa9f6-703e-4a93-ab8b-a6d7ad40628b?lawId=16b...\n",
            "\n",
            "๐น Result ID 5 (score=0.370)\n",
            "ุงููุงุฏุฉ ุงูุณุงุฏุณุฉ - ูุจุงูุน ุงูููุงุทููู\n",
            "ุงูููู\n",
            "ุนูู\n",
            "ูุชุงุจ ุงููู ุชุนุงูู\n",
            "ุ ูุณูุฉ ุฑุณูููุ ูุนูู ุงูุณูุน ูุงูุทุงุนุฉ ูู ุงูุนุณุฑ ูุงููุณุฑ ูุงูููุดุท ูุงูููุฑู. ุงูุงุณู: ุงููุธุงู ุงูุฃุณุงุณู ููุญูู ุชุงุฑูุฎ ุงูุฅุตุฏุงุฑ: 1412/08/27 ูู  ุงูููุงูู : 01/03/1992 ูู ุชุงุฑูุฎ ุงููุดุฑ: 1412/09/02  ูู ุงูููุงูู : 06/03/1992 ูู ุงูุญุงูุฉ: ุณุงุฑู ุฃุฏูุงุช ุฅุตุฏุงุฑ ุงููุธุงู: [{'text': 'ุฃูุฑ ูููู ุฑูู ุฃ/90 ุจุชุงุฑูุฎ 27 / 8 / 1412', 'url': 'https://laws.boe.gov.sa/BoeLaws/Laws/Viewer/0c1f...\n",
            "\n",
            "๐น Result ID -1 (score=-340282346638528859811704183484516925440.000)\n",
            "ุงููุงุฏุฉ ุงูุซุงูุซุฉ ุนุดุฑุฉ - ุชูุดุฑ ุงูุชุฑุชูุจุงุช ูู ุงูุฌุฑูุฏุฉ ุงูุฑุณููุฉุ ููุนูู ุจูุง ูู ุชุงุฑูุฎ ูุดุฑูุง. ุงูุงุณู: ุงูุชุฑุชูุจุงุช ุงูุชูุธูููุฉ ููุฑูุฒ ุงููุนุงููุงุช (ุงููุฑูุฒ ุงููุทูู ูููุนุงููุงุช) ุชุงุฑูุฎ ุงูุฅุตุฏุงุฑ: 1441/07/15 ูู  ุงูููุงูู : 10/03/2020 ูู ุชุงุฑูุฎ ุงููุดุฑ: ูู ูุชู ุชุญุฏูุฏ ุชุงุฑูุฎ ุงููุดุฑ ุงูุญุงูุฉ: ุฌุงุฑู ุงูุนูู ุนูู ุงููุธุงู ุฃุฏูุงุช ุฅุตุฏุงุฑ ุงููุธุงู: [{'text': 'ูุฑุงุฑ ูุฌูุณ ุงููุฒุฑุงุก ุฑูู (471) ูุชุงุฑูุฎ 1441/7/15ูู', 'url': 'https://laws.boe.gov.sa/BoeLaws/Laws/V...\n"
          ]
        }
      ],
      "source": [
        "filtered_indices = np.array([0, 1,2, 5 ], dtype=np.int64)\n",
        "selector = faiss.IDSelectorArray(filtered_indices)\n",
        "def retrieve(query, top_k=5):\n",
        "    q_emb = embed_model.encode([query], convert_to_numpy=True, normalize_embeddings=True)\n",
        "    D, I = index.search(q_emb, top_k, params=faiss.SearchParameters(sel=selector)\n",
        ")\n",
        "    results = [(i, float(D[0][j])) for j, i in enumerate(I[0])]\n",
        "    return results\n",
        "query = \"ูุง ูู ุดุนุงุฑ ุงูุฏููุฉ ุงูุณุนูุฏูุฉุ\"\n",
        "results = retrieve(query)\n",
        "for i, (idx, score) in enumerate(results, 1):\n",
        "    print(f\"\\n๐น Result ID {idx} (score={score:.3f})\\n{corpus[idx][:400]}...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "โ Laws corpus built with 517 entries\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'Law Name:ุงููุธุงู ุงูุฃุณุงุณู ููุญูู\\nLaw Summary:ูุชุถูู ุงูุนูุงููู ุงูุชุงููุฉ: ุงููุจุงุฏุฆ ุงูุนุงูุฉุ ูุธุงู ุงูุญููุ ููููุงุช ุงููุฌุชูุน ุงูุณุนูุฏูุ ุงููุจุงุฏุฆ ุงูุงูุชุตุงุฏูุฉุ ุงูุญููู ูุงููุงุฌุจุงุชุ ุณูุทุงุช ุงูุฏููุฉุ ุงูุดุฆูู ุงููุงููุฉุ ุฃุญูุงู ุนุงูุฉ.'"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def build_law_corpus(data):\n",
        "    laws_text = []\n",
        "    # Traverse the 3-level hierarchy\n",
        "    for main_cat_name, sub_categories in data.items():\n",
        "        for sub_cat_name, laws in sub_categories.items():\n",
        "            for law_title, law_data in laws.items():\n",
        "                laws_text.append(\"Law Name:\"+ law_title+\"\\nLaw Summary:\"+law_data['brief'] )\n",
        "                \n",
        "    return laws_text\n",
        "\n",
        "laws_corpus = build_law_corpus(data)\n",
        "print(f\"โ Laws corpus built with {len(laws_corpus)} entries\")\n",
        "laws_corpus[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|โโโโโโโโโโ| 5/5 [00:03<00:00,  1.54it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "โ Laws corpus embeddings shape: (517, 1024)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "embeddings = embed_corpus(laws_corpus, embed_model)\n",
        "print(f\"โ Laws corpus embeddings shape: {embeddings.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "law_index = save_faiss_index(embeddings, index_path=\"laws_legal_faiss.index\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_parts_corpus(data, max_tokens=512, chunk_overlap=50, tokenizer_func=None):\n",
        "    def count_tokens(text):\n",
        "        \"\"\"Simple token counting - you might want to use a proper tokenizer\"\"\"\n",
        "        return len(tokenizer_func(text, add_special_tokens=False))\n",
        "    \n",
        "    def chunk_text(text, max_tokens, overlap):\n",
        "        \"\"\"Split text into chunks based on token count while preserving context\"\"\"\n",
        "        sentences = text.split('. ')\n",
        "        chunks = []\n",
        "        current_chunk = []\n",
        "        current_token_count = 0\n",
        "        \n",
        "        for sentence in sentences:\n",
        "            sentence_tokens = count_tokens(sentence)\n",
        "            \n",
        "            # If adding this sentence would exceed the limit\n",
        "            if current_token_count + sentence_tokens > max_tokens and current_chunk:\n",
        "                # Save current chunk\n",
        "                chunks.append('. '.join(current_chunk).strip())\n",
        "                \n",
        "                # Start new chunk with overlap\n",
        "                if overlap > 0:\n",
        "                    # Calculate how many sentences to keep for overlap\n",
        "                    overlap_text = '. '.join(current_chunk[-3:]).strip()  # Rough overlap\n",
        "                    current_chunk = [overlap_text] if overlap_text else []\n",
        "                    current_token_count = count_tokens(overlap_text) if overlap_text else 0\n",
        "                else:\n",
        "                    current_chunk = []\n",
        "                    current_token_count = 0\n",
        "            \n",
        "            current_chunk.append(sentence)\n",
        "            current_token_count += sentence_tokens\n",
        "        \n",
        "        # Add the last chunk if it has content\n",
        "        if current_chunk:\n",
        "            chunks.append('. '.join(current_chunk).strip())\n",
        "        \n",
        "        return chunks\n",
        "\n",
        "    def format_article(article):\n",
        "        title = article.get(\"Article_Title\", \"\").strip()\n",
        "        text = article.get(\"Article_Text\", \"\").strip()\n",
        "        parts = []\n",
        "        if title:\n",
        "            parts.append(f\"Article Title: {title}\")\n",
        "        if text:\n",
        "            parts.append(f\"Article Text: {text}\")\n",
        "        return \"\\n\".join(parts)\n",
        "    \n",
        "    corpus = []  # Will store dicts instead of plain text\n",
        "    id_to_part_id = {}  \n",
        "    chunk_id = 0\n",
        "    # Traverse the 3-level hierarchy\n",
        "    for main_cat_name, sub_categories in data.items():\n",
        "        for sub_cat_name, laws in sub_categories.items():\n",
        "            for law_title, law_data in laws.items():\n",
        "                parts = law_data.get(\"parts\", {})\n",
        "                brief = law_data.get(\"brief\", \"\")\n",
        "                law_text = \"Law Title: \" + law_title + \"\\n\" + \"Law Brief: \" + brief\n",
        "                \n",
        "                for part_name, part_articles in parts.items():\n",
        "                    # Build the part context\n",
        "                    if part_name != \"main\":\n",
        "                        context_lines = [law_text, f\"Part Name: {part_name}\"]\n",
        "                    else:\n",
        "                        context_lines = [law_text]\n",
        "                    \n",
        "                    context_lines.extend([format_article(article) for article in part_articles])\n",
        "                    full_text = \"\\n\".join(context_lines)\n",
        "                    \n",
        "                    part_id = law_title + \"|\" + part_name \n",
        "                   \n",
        "                    # Check token count\n",
        "                    token_count = count_tokens(full_text)\n",
        "                    if token_count > max_tokens:\n",
        "                        # Chunk the text and track chunk indices\n",
        "                        chunked_texts = chunk_text(full_text, max_tokens, chunk_overlap)\n",
        "                        \n",
        "                        for idx, chunk in enumerate(chunked_texts):\n",
        "                            # Add chunk to corpus with unique ID\n",
        "                            corpus.append(chunk)\n",
        "                            id_to_part_id[chunk_id] = part_id\n",
        "                            chunk_id += 1\n",
        "                    else:\n",
        "                        # Create unique ID for non-chunked part\n",
        "                        corpus.append(full_text)\n",
        "                        id_to_part_id[chunk_id] = part_id\n",
        "                        chunk_id += 1\n",
        "                        \n",
        "\n",
        "\n",
        "    return {\n",
        "        \"corpus\": corpus,  \n",
        "        \"corpus_id_to_part_id\": id_to_part_id  \n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "8192"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "embed_model.tokenizer.model_max_length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "โ Parts corpus built with 2050 entries\n"
          ]
        }
      ],
      "source": [
        "parts_data = build_parts_corpus(data, max_tokens=8192, chunk_overlap=50, tokenizer_func=embed_model.tokenizer.encode)\n",
        "corpus = parts_data['corpus']\n",
        "print(f\"โ Parts corpus built with {len(corpus)} entries\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2050"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(parts_data['corpus_id_to_part_id'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|โโโโโโโโโโ| 129/129 [12:01<00:00,  5.60s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "โ Parts corpus embeddings shape: (2050, 1024)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "embeddings = embed_corpus(corpus, embed_model, batch_size=16)\n",
        "print(f\"โ Parts corpus embeddings shape: {embeddings.shape}\")\n",
        "index = save_faiss_index(embeddings, index_path=\"m3_legal_faiss_parts.index\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "json.dump(parts_data['corpus_id_to_part_id'], open(\"m3_corpus_id_to_part_id_parts.json\", \"w\", encoding=\"utf-8\"), ensure_ascii=False, indent=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "legal-assistant",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
